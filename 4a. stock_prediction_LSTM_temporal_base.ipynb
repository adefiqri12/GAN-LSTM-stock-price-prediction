{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d950ded-1027-4e45-8b25-8f24281348c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T08:46:09.685669Z",
     "iopub.status.busy": "2024-11-25T08:46:09.685257Z",
     "iopub.status.idle": "2024-11-25T08:46:09.691381Z",
     "shell.execute_reply": "2024-11-25T08:46:09.690301Z",
     "shell.execute_reply.started": "2024-11-25T08:46:09.685643Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 440 µs (started: 2024-11-25 08:46:09 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# \"\"\"\n",
    "# predicting a sequence of future prices (`n_steps_out`) instead of just one.\n",
    "\n",
    "# Benefits of This Approach\n",
    "# Utilizes Temporal Patterns: The model learns dependencies across multiple future time steps.\n",
    "# More Robust Predictions: Predicting a sequence captures trends better than single-step predictions.\n",
    "# Real-World Applicability: Multi-step forecasting is closer to real-world use cases, such as portfolio management.\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ba2dc6c-a4c3-4858-8746-10fd762da60f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T08:46:09.893658Z",
     "iopub.status.busy": "2024-11-25T08:46:09.892479Z",
     "iopub.status.idle": "2024-11-25T08:46:17.231363Z",
     "shell.execute_reply": "2024-11-25T08:46:17.230481Z",
     "shell.execute_reply.started": "2024-11-25T08:46:09.893612Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mtime: 7.33 s (started: 2024-11-25 08:46:09 +00:00)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt -q\n",
    "!pip install ipython-autotime -q\n",
    "# !pip install tensorflow-addons -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5f4333d-89b9-4595-af27-fe9811842cdf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T08:46:17.233989Z",
     "iopub.status.busy": "2024-11-25T08:46:17.233650Z",
     "iopub.status.idle": "2024-11-25T08:46:17.247897Z",
     "shell.execute_reply": "2024-11-25T08:46:17.247152Z",
     "shell.execute_reply.started": "2024-11-25T08:46:17.233961Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autotime extension is already loaded. To reload it, use:\n",
      "  %reload_ext autotime\n",
      "tensorflow version: 2.15.0\n",
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 7.09 ms (started: 2024-11-25 08:46:17 +00:00)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from keras.layers import Input, Dense, Dropout, BatchNormalization, Reshape, Flatten, Bidirectional, LSTM\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autotime\n",
    "\n",
    "print(\"tensorflow version:\", tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "tf.test.is_built_with_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a131d3ef-8a5d-4d18-b97b-1636caf46390",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T08:46:17.249717Z",
     "iopub.status.busy": "2024-11-25T08:46:17.249067Z",
     "iopub.status.idle": "2024-11-25T08:46:17.253885Z",
     "shell.execute_reply": "2024-11-25T08:46:17.253109Z",
     "shell.execute_reply.started": "2024-11-25T08:46:17.249691Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 580 µs (started: 2024-11-25 08:46:17 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# parameter\n",
    "# n_steps_in: Number of input time steps\n",
    "# n_steps_out: Number of days to predict ahead\n",
    "\n",
    "n_steps_in = 14\n",
    "n_steps_out = 5\n",
    "epochs = 200\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5f71067-767a-46a5-80a7-215a24bdabf2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T08:46:17.256539Z",
     "iopub.status.busy": "2024-11-25T08:46:17.256021Z",
     "iopub.status.idle": "2024-11-25T08:46:17.386018Z",
     "shell.execute_reply": "2024-11-25T08:46:17.385112Z",
     "shell.execute_reply.started": "2024-11-25T08:46:17.256511Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 125 ms (started: 2024-11-25 08:46:17 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def load_processed_data(data_dir='processed_data', df_name='', array_name=''):\n",
    "    \"\"\"\n",
    "    Load processed DataFrames list and numpy arrays from files\n",
    "    \"\"\"\n",
    "    data_path = Path(data_dir)\n",
    "    \n",
    "    # Load list of DataFrames\n",
    "    with open(data_path / df_name, 'rb') as f:\n",
    "        processed_dfs = joblib.load(f)\n",
    "    \n",
    "    # Load numpy arrays\n",
    "    with open(data_path / array_name, 'rb') as f:\n",
    "        values_list = joblib.load(f)\n",
    "    \n",
    "    return processed_dfs, values_list\n",
    "\n",
    "processed_dfs, values_list = load_processed_data(df_name='processed_dfs.pkl', array_name='values_array.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e426c2fb-215a-40bf-8d5b-a4a9769378ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T08:46:17.387529Z",
     "iopub.status.busy": "2024-11-25T08:46:17.387246Z",
     "iopub.status.idle": "2024-11-25T08:46:17.432821Z",
     "shell.execute_reply": "2024-11-25T08:46:17.431789Z",
     "shell.execute_reply.started": "2024-11-25T08:46:17.387498Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 40.3 ms (started: 2024-11-25 08:46:17 +00:00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41/1811191807.py:4: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  processed_dfs[i].drop(columns=['Volume', 'RSI', 'WR','Price Momentum', 'Volume Momentum', 'CCI', 'Ulcer Index'],inplace=True)\n"
     ]
    }
   ],
   "source": [
    "values_list = []\n",
    "for i in range(len(processed_dfs)):\n",
    "    # processed_dfs[i].drop(processed_dfs[i].columns[1:5], axis=1, inplace=True)\n",
    "    processed_dfs[i].drop(columns=['Volume', 'RSI', 'WR','Price Momentum', 'Volume Momentum', 'CCI', 'Ulcer Index'],inplace=True)\n",
    "    x = processed_dfs[i].values\n",
    "    values_list.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc07535b-1dc8-461d-a7ed-f32c1a976ed6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T08:46:17.434612Z",
     "iopub.status.busy": "2024-11-25T08:46:17.433974Z",
     "iopub.status.idle": "2024-11-25T08:46:17.463468Z",
     "shell.execute_reply": "2024-11-25T08:46:17.462594Z",
     "shell.execute_reply.started": "2024-11-25T08:46:17.434585Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Price</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>SD20</th>\n",
       "      <th>SMA7</th>\n",
       "      <th>SMA14</th>\n",
       "      <th>SMA21</th>\n",
       "      <th>EMA7</th>\n",
       "      <th>...</th>\n",
       "      <th>Lower Band</th>\n",
       "      <th>ARIMA Output</th>\n",
       "      <th>Fourier Power</th>\n",
       "      <th>Fourier Phase</th>\n",
       "      <th>Fourier 3 Real</th>\n",
       "      <th>Fourier 6 Real</th>\n",
       "      <th>Fourier 9 Real</th>\n",
       "      <th>Fourier 100 Real</th>\n",
       "      <th>Fourier Significant real</th>\n",
       "      <th>Close Next Day</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticker</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>AAPL</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>...</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011-08-16</th>\n",
       "      <td>11.462601</td>\n",
       "      <td>13.588571</td>\n",
       "      <td>13.691786</td>\n",
       "      <td>13.430714</td>\n",
       "      <td>13.625714</td>\n",
       "      <td>0.441967</td>\n",
       "      <td>13.293316</td>\n",
       "      <td>13.563801</td>\n",
       "      <td>13.700425</td>\n",
       "      <td>13.479375</td>\n",
       "      <td>...</td>\n",
       "      <td>12.850948</td>\n",
       "      <td>11.386598</td>\n",
       "      <td>564.736488</td>\n",
       "      <td>1.784183</td>\n",
       "      <td>50.608489</td>\n",
       "      <td>10.548855</td>\n",
       "      <td>-3.418024</td>\n",
       "      <td>12.356305</td>\n",
       "      <td>16.000357</td>\n",
       "      <td>13.587143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-08-17</th>\n",
       "      <td>11.461396</td>\n",
       "      <td>13.587143</td>\n",
       "      <td>13.732857</td>\n",
       "      <td>13.500000</td>\n",
       "      <td>13.653929</td>\n",
       "      <td>0.442083</td>\n",
       "      <td>13.432245</td>\n",
       "      <td>13.534770</td>\n",
       "      <td>13.706531</td>\n",
       "      <td>13.506317</td>\n",
       "      <td>...</td>\n",
       "      <td>12.839185</td>\n",
       "      <td>11.624125</td>\n",
       "      <td>497.700387</td>\n",
       "      <td>1.823144</td>\n",
       "      <td>50.372918</td>\n",
       "      <td>10.291735</td>\n",
       "      <td>-3.410067</td>\n",
       "      <td>12.751208</td>\n",
       "      <td>15.711276</td>\n",
       "      <td>13.073214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-08-18</th>\n",
       "      <td>11.027875</td>\n",
       "      <td>13.073214</td>\n",
       "      <td>13.308929</td>\n",
       "      <td>12.906071</td>\n",
       "      <td>13.244286</td>\n",
       "      <td>0.462345</td>\n",
       "      <td>13.391633</td>\n",
       "      <td>13.472449</td>\n",
       "      <td>13.671071</td>\n",
       "      <td>13.398041</td>\n",
       "      <td>...</td>\n",
       "      <td>12.761759</td>\n",
       "      <td>11.368409</td>\n",
       "      <td>645.802434</td>\n",
       "      <td>1.591148</td>\n",
       "      <td>50.137814</td>\n",
       "      <td>10.037805</td>\n",
       "      <td>-3.396865</td>\n",
       "      <td>13.168186</td>\n",
       "      <td>15.424764</td>\n",
       "      <td>12.715357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-08-19</th>\n",
       "      <td>10.726002</td>\n",
       "      <td>12.715357</td>\n",
       "      <td>13.107143</td>\n",
       "      <td>12.714286</td>\n",
       "      <td>12.934643</td>\n",
       "      <td>0.498631</td>\n",
       "      <td>13.352551</td>\n",
       "      <td>13.368571</td>\n",
       "      <td>13.617908</td>\n",
       "      <td>13.227370</td>\n",
       "      <td>...</td>\n",
       "      <td>12.624472</td>\n",
       "      <td>11.025292</td>\n",
       "      <td>707.291371</td>\n",
       "      <td>1.742971</td>\n",
       "      <td>49.903177</td>\n",
       "      <td>9.787063</td>\n",
       "      <td>-3.378466</td>\n",
       "      <td>13.593889</td>\n",
       "      <td>15.140825</td>\n",
       "      <td>12.730000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-08-22</th>\n",
       "      <td>10.738355</td>\n",
       "      <td>12.730000</td>\n",
       "      <td>13.031429</td>\n",
       "      <td>12.681786</td>\n",
       "      <td>13.018214</td>\n",
       "      <td>0.510826</td>\n",
       "      <td>13.264490</td>\n",
       "      <td>13.285740</td>\n",
       "      <td>13.555221</td>\n",
       "      <td>13.103027</td>\n",
       "      <td>...</td>\n",
       "      <td>12.525591</td>\n",
       "      <td>10.494964</td>\n",
       "      <td>569.391019</td>\n",
       "      <td>1.997446</td>\n",
       "      <td>49.669010</td>\n",
       "      <td>9.539509</td>\n",
       "      <td>-3.354922</td>\n",
       "      <td>14.014846</td>\n",
       "      <td>14.859461</td>\n",
       "      <td>13.342857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-24</th>\n",
       "      <td>230.316620</td>\n",
       "      <td>230.570007</td>\n",
       "      <td>230.820007</td>\n",
       "      <td>228.410004</td>\n",
       "      <td>229.979996</td>\n",
       "      <td>3.892879</td>\n",
       "      <td>233.228570</td>\n",
       "      <td>230.810000</td>\n",
       "      <td>229.767143</td>\n",
       "      <td>232.353797</td>\n",
       "      <td>...</td>\n",
       "      <td>222.290882</td>\n",
       "      <td>230.674707</td>\n",
       "      <td>21857.804244</td>\n",
       "      <td>-1.509946</td>\n",
       "      <td>105.115706</td>\n",
       "      <td>115.104123</td>\n",
       "      <td>117.980892</td>\n",
       "      <td>188.509112</td>\n",
       "      <td>114.330593</td>\n",
       "      <td>231.410004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-25</th>\n",
       "      <td>231.155685</td>\n",
       "      <td>231.410004</td>\n",
       "      <td>233.220001</td>\n",
       "      <td>229.570007</td>\n",
       "      <td>229.740005</td>\n",
       "      <td>3.874728</td>\n",
       "      <td>233.175714</td>\n",
       "      <td>231.504286</td>\n",
       "      <td>229.952381</td>\n",
       "      <td>232.117849</td>\n",
       "      <td>...</td>\n",
       "      <td>222.507266</td>\n",
       "      <td>231.297362</td>\n",
       "      <td>30163.314500</td>\n",
       "      <td>-1.569032</td>\n",
       "      <td>104.839212</td>\n",
       "      <td>114.481354</td>\n",
       "      <td>117.032685</td>\n",
       "      <td>177.737174</td>\n",
       "      <td>113.773702</td>\n",
       "      <td>233.399994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-28</th>\n",
       "      <td>233.143494</td>\n",
       "      <td>233.399994</td>\n",
       "      <td>234.729996</td>\n",
       "      <td>232.550003</td>\n",
       "      <td>233.320007</td>\n",
       "      <td>3.891694</td>\n",
       "      <td>233.354285</td>\n",
       "      <td>232.049285</td>\n",
       "      <td>230.219524</td>\n",
       "      <td>232.438385</td>\n",
       "      <td>...</td>\n",
       "      <td>222.494192</td>\n",
       "      <td>231.265411</td>\n",
       "      <td>26178.111649</td>\n",
       "      <td>-1.048381</td>\n",
       "      <td>104.562621</td>\n",
       "      <td>113.857934</td>\n",
       "      <td>116.083356</td>\n",
       "      <td>166.331105</td>\n",
       "      <td>113.216242</td>\n",
       "      <td>233.669998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-29</th>\n",
       "      <td>233.413193</td>\n",
       "      <td>233.669998</td>\n",
       "      <td>234.330002</td>\n",
       "      <td>232.320007</td>\n",
       "      <td>233.100006</td>\n",
       "      <td>3.858562</td>\n",
       "      <td>233.164285</td>\n",
       "      <td>232.344285</td>\n",
       "      <td>230.251429</td>\n",
       "      <td>232.746288</td>\n",
       "      <td>...</td>\n",
       "      <td>222.931777</td>\n",
       "      <td>231.813840</td>\n",
       "      <td>25255.645241</td>\n",
       "      <td>-1.559314</td>\n",
       "      <td>104.285935</td>\n",
       "      <td>113.233893</td>\n",
       "      <td>115.132989</td>\n",
       "      <td>154.405762</td>\n",
       "      <td>112.658234</td>\n",
       "      <td>230.100006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-30</th>\n",
       "      <td>229.847122</td>\n",
       "      <td>230.100006</td>\n",
       "      <td>233.470001</td>\n",
       "      <td>229.550003</td>\n",
       "      <td>232.610001</td>\n",
       "      <td>3.762423</td>\n",
       "      <td>232.252858</td>\n",
       "      <td>232.420000</td>\n",
       "      <td>230.436667</td>\n",
       "      <td>232.084718</td>\n",
       "      <td>...</td>\n",
       "      <td>223.285187</td>\n",
       "      <td>233.709615</td>\n",
       "      <td>63404.740343</td>\n",
       "      <td>-1.633357</td>\n",
       "      <td>104.009157</td>\n",
       "      <td>112.609258</td>\n",
       "      <td>114.181670</td>\n",
       "      <td>142.086246</td>\n",
       "      <td>112.099699</td>\n",
       "      <td>225.910004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3324 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Price        Adj Close       Close        High         Low        Open  \\\n",
       "Ticker            AAPL        AAPL        AAPL        AAPL        AAPL   \n",
       "Date                                                                     \n",
       "2011-08-16   11.462601   13.588571   13.691786   13.430714   13.625714   \n",
       "2011-08-17   11.461396   13.587143   13.732857   13.500000   13.653929   \n",
       "2011-08-18   11.027875   13.073214   13.308929   12.906071   13.244286   \n",
       "2011-08-19   10.726002   12.715357   13.107143   12.714286   12.934643   \n",
       "2011-08-22   10.738355   12.730000   13.031429   12.681786   13.018214   \n",
       "...                ...         ...         ...         ...         ...   \n",
       "2024-10-24  230.316620  230.570007  230.820007  228.410004  229.979996   \n",
       "2024-10-25  231.155685  231.410004  233.220001  229.570007  229.740005   \n",
       "2024-10-28  233.143494  233.399994  234.729996  232.550003  233.320007   \n",
       "2024-10-29  233.413193  233.669998  234.330002  232.320007  233.100006   \n",
       "2024-10-30  229.847122  230.100006  233.470001  229.550003  232.610001   \n",
       "\n",
       "Price           SD20        SMA7       SMA14       SMA21        EMA7  ...  \\\n",
       "Ticker                                                                ...   \n",
       "Date                                                                  ...   \n",
       "2011-08-16  0.441967   13.293316   13.563801   13.700425   13.479375  ...   \n",
       "2011-08-17  0.442083   13.432245   13.534770   13.706531   13.506317  ...   \n",
       "2011-08-18  0.462345   13.391633   13.472449   13.671071   13.398041  ...   \n",
       "2011-08-19  0.498631   13.352551   13.368571   13.617908   13.227370  ...   \n",
       "2011-08-22  0.510826   13.264490   13.285740   13.555221   13.103027  ...   \n",
       "...              ...         ...         ...         ...         ...  ...   \n",
       "2024-10-24  3.892879  233.228570  230.810000  229.767143  232.353797  ...   \n",
       "2024-10-25  3.874728  233.175714  231.504286  229.952381  232.117849  ...   \n",
       "2024-10-28  3.891694  233.354285  232.049285  230.219524  232.438385  ...   \n",
       "2024-10-29  3.858562  233.164285  232.344285  230.251429  232.746288  ...   \n",
       "2024-10-30  3.762423  232.252858  232.420000  230.436667  232.084718  ...   \n",
       "\n",
       "Price       Lower Band ARIMA Output Fourier Power Fourier Phase  \\\n",
       "Ticker                                                            \n",
       "Date                                                              \n",
       "2011-08-16   12.850948    11.386598    564.736488      1.784183   \n",
       "2011-08-17   12.839185    11.624125    497.700387      1.823144   \n",
       "2011-08-18   12.761759    11.368409    645.802434      1.591148   \n",
       "2011-08-19   12.624472    11.025292    707.291371      1.742971   \n",
       "2011-08-22   12.525591    10.494964    569.391019      1.997446   \n",
       "...                ...          ...           ...           ...   \n",
       "2024-10-24  222.290882   230.674707  21857.804244     -1.509946   \n",
       "2024-10-25  222.507266   231.297362  30163.314500     -1.569032   \n",
       "2024-10-28  222.494192   231.265411  26178.111649     -1.048381   \n",
       "2024-10-29  222.931777   231.813840  25255.645241     -1.559314   \n",
       "2024-10-30  223.285187   233.709615  63404.740343     -1.633357   \n",
       "\n",
       "Price      Fourier 3 Real Fourier 6 Real Fourier 9 Real Fourier 100 Real  \\\n",
       "Ticker                                                                     \n",
       "Date                                                                       \n",
       "2011-08-16      50.608489      10.548855      -3.418024        12.356305   \n",
       "2011-08-17      50.372918      10.291735      -3.410067        12.751208   \n",
       "2011-08-18      50.137814      10.037805      -3.396865        13.168186   \n",
       "2011-08-19      49.903177       9.787063      -3.378466        13.593889   \n",
       "2011-08-22      49.669010       9.539509      -3.354922        14.014846   \n",
       "...                   ...            ...            ...              ...   \n",
       "2024-10-24     105.115706     115.104123     117.980892       188.509112   \n",
       "2024-10-25     104.839212     114.481354     117.032685       177.737174   \n",
       "2024-10-28     104.562621     113.857934     116.083356       166.331105   \n",
       "2024-10-29     104.285935     113.233893     115.132989       154.405762   \n",
       "2024-10-30     104.009157     112.609258     114.181670       142.086246   \n",
       "\n",
       "Price      Fourier Significant real Close Next Day  \n",
       "Ticker                                              \n",
       "Date                                                \n",
       "2011-08-16                16.000357      13.587143  \n",
       "2011-08-17                15.711276      13.073214  \n",
       "2011-08-18                15.424764      12.715357  \n",
       "2011-08-19                15.140825      12.730000  \n",
       "2011-08-22                14.859461      13.342857  \n",
       "...                             ...            ...  \n",
       "2024-10-24               114.330593     231.410004  \n",
       "2024-10-25               113.773702     233.399994  \n",
       "2024-10-28               113.216242     233.669998  \n",
       "2024-10-29               112.658234     230.100006  \n",
       "2024-10-30               112.099699     225.910004  \n",
       "\n",
       "[3324 rows x 26 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 25.6 ms (started: 2024-11-25 08:46:17 +00:00)\n"
     ]
    }
   ],
   "source": [
    "processed_dfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ad6ccf5-ec0c-4604-bd56-1d8aa11e6768",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T08:46:17.465300Z",
     "iopub.status.busy": "2024-11-25T08:46:17.465038Z",
     "iopub.status.idle": "2024-11-25T08:46:17.899717Z",
     "shell.execute_reply": "2024-11-25T08:46:17.898879Z",
     "shell.execute_reply.started": "2024-11-25T08:46:17.465277Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training shapes: X=(66115, 14, 25), y=(66115, 5)\n",
      "Validation shapes: X=(16184, 14, 25), y=(16184, 5)\n",
      "Number of features: 25\n",
      "time: 424 ms (started: 2024-11-25 08:46:17 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_stock_data(values_list, n_steps_in=14, n_steps_out=5, train_split=0.8):\n",
    "    \"\"\"\n",
    "    Preprocess multiple stock datasets for LSTM training.\n",
    "    \n",
    "    Args:\n",
    "        values_list: List of numpy arrays containing stock data with technical indicators\n",
    "        n_steps_in: Number of lookback days\n",
    "        n_steps_out: Number of prediction days\n",
    "        train_split: Train/validation split ratio\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (train_X, train_y, val_X, val_y, global_scaler)\n",
    "    \"\"\"\n",
    "    # 1. Global scaling across all stocks\n",
    "    global_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    combined_values = np.vstack(values_list)\n",
    "    scaled_combined = global_scaler.fit_transform(combined_values)\n",
    "    \n",
    "    # 2. Split back into individual stocks\n",
    "    scaled_values_list = []\n",
    "    start_idx = 0\n",
    "    for values in values_list:\n",
    "        scaled_values_list.append(scaled_combined[start_idx:start_idx + len(values)])\n",
    "        start_idx += len(values)\n",
    "    \n",
    "    # 3. Create sequences for each stock\n",
    "    train_X_list, train_y_list = [], []\n",
    "    val_X_list, val_y_list = [], []\n",
    "    \n",
    "    for scaled_values in scaled_values_list:\n",
    "        # Remove 'Close Next Day' from features (last column)\n",
    "        features = scaled_values[:, :-1]  # All columns except the last one\n",
    "        targets = scaled_values[:, -1]    # Only the last column\n",
    "        \n",
    "        # Split into train/validation\n",
    "        n_train = int(len(features) * train_split)\n",
    "        \n",
    "        # Ensure we have enough data for both training and validation\n",
    "        if n_train <= n_steps_in + n_steps_out:\n",
    "            print(f\"Warning: Stock with {len(features)} samples is too short for meaningful splitting\")\n",
    "            continue\n",
    "            \n",
    "        # Split features and targets\n",
    "        train_features = features[:n_train]\n",
    "        train_targets = targets[:n_train]\n",
    "        val_features = features[n_train:]\n",
    "        val_targets = targets[n_train:]\n",
    "        \n",
    "        # Create sequences\n",
    "        if len(train_features) > n_steps_in + n_steps_out:\n",
    "            train_X, train_y = create_sequences(train_features, train_targets, n_steps_in, n_steps_out)\n",
    "            train_X_list.append(train_X)\n",
    "            train_y_list.append(train_y)\n",
    "            \n",
    "        if len(val_features) > n_steps_in + n_steps_out:\n",
    "            val_X, val_y = create_sequences(val_features, val_targets, n_steps_in, n_steps_out)\n",
    "            val_X_list.append(val_X)\n",
    "            val_y_list.append(val_y)\n",
    "    \n",
    "    # 4. Combine all sequences\n",
    "    train_X = np.vstack(train_X_list)\n",
    "    train_y = np.vstack(train_y_list)\n",
    "    val_X = np.vstack(val_X_list)\n",
    "    val_y = np.vstack(val_y_list)\n",
    "    \n",
    "    print(f\"Training shapes: X={train_X.shape}, y={train_y.shape}\")\n",
    "    print(f\"Validation shapes: X={val_X.shape}, y={val_y.shape}\")\n",
    "    print(f\"Number of features: {train_X.shape[2]}\")\n",
    "    \n",
    "    return train_X, train_y, val_X, val_y, global_scaler\n",
    "\n",
    "def create_sequences(features, targets, n_steps_in, n_steps_out):\n",
    "    \"\"\"\n",
    "    Generate synchronized sequences for LSTM input features and output targets.\n",
    "    \n",
    "    Args:\n",
    "        features: Scaled feature data (numpy array)\n",
    "        targets: Scaled target data (numpy array)\n",
    "        n_steps_in: Number of input time steps\n",
    "        n_steps_out: Number of output time steps\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (X sequences, y sequences)\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    \n",
    "    # Ensure we have enough data for sequence creation\n",
    "    if len(features) < n_steps_in + n_steps_out:\n",
    "        raise ValueError(\"Data length is too short for the specified sequence lengths\")\n",
    "    \n",
    "    for i in range(len(features) - n_steps_in - n_steps_out + 1):\n",
    "        # Input sequence (n_steps_in days of all features)\n",
    "        seq_x = features[i:(i + n_steps_in)]\n",
    "        # Output sequence (next n_steps_out days of target variable)\n",
    "        seq_y = targets[(i + n_steps_in):(i + n_steps_in + n_steps_out)]\n",
    "        \n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "train_X, train_y, val_X, val_y, scaler = preprocess_stock_data(values_list, n_steps_in, n_steps_out, train_split=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d086cc3e-82d4-4fa7-9116-4c2a0598bef2",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "84421eaf-7f87-4409-b27d-1bafd23e1691",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T08:46:17.903738Z",
     "iopub.status.busy": "2024-11-25T08:46:17.902819Z",
     "iopub.status.idle": "2024-11-25T08:46:18.135657Z",
     "shell.execute_reply": "2024-11-25T08:46:18.134704Z",
     "shell.execute_reply.started": "2024-11-25T08:46:17.903712Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 200)               180800    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 5)                 1005      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 181805 (710.18 KB)\n",
      "Trainable params: 181805 (710.18 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "time: 228 ms (started: 2024-11-25 08:46:17 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow_addons as tfa\n",
    "\n",
    "# Cyclical learning rate\n",
    "# steps_per_epoch = len(train_X) // batch_size\n",
    "# n = 2\n",
    "# cycle = 12\n",
    "# epochs = cycle * n * 2\n",
    "# print(epochs)\n",
    "\n",
    "# clr = tfa.optimizers.TriangularCyclicalLearningRate(\n",
    "#     initial_learning_rate=1e-4,\n",
    "#     maximal_learning_rate=1e-2,\n",
    "#     step_size = n * steps_per_epoch)\n",
    "\n",
    "def lstm_model(n_steps_in, n_features, n_steps_out):\n",
    "    tf.keras.backend.clear_session()\n",
    "    model = Sequential([\n",
    "        LSTM(200, activation='relu', dropout=0.025, input_shape=(n_steps_in, n_features)),\n",
    "        Dense(n_steps_out)\n",
    "    ])\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "    loss = ['mae']\n",
    "    model.compile(optimizer=optimizer, loss_weights=[1], loss=loss)\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create and train model\n",
    "model = lstm_model(n_steps_in=n_steps_in, n_features=train_X.shape[2], n_steps_out=n_steps_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55a2135-96f3-404e-9999-b0bdf0a115cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T08:46:18.137533Z",
     "iopub.status.busy": "2024-11-25T08:46:18.137133Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1034/1034 - 24s - loss: 0.0358 - val_loss: 0.0195 - 24s/epoch - 23ms/step\n",
      "Epoch 2/200\n",
      "1034/1034 - 22s - loss: 0.0076 - val_loss: 0.0160 - 22s/epoch - 21ms/step\n",
      "Epoch 3/200\n",
      "1034/1034 - 21s - loss: 0.0059 - val_loss: 0.0148 - 21s/epoch - 21ms/step\n",
      "Epoch 4/200\n",
      "1034/1034 - 21s - loss: 0.0054 - val_loss: 0.0140 - 21s/epoch - 20ms/step\n",
      "Epoch 5/200\n",
      "1034/1034 - 20s - loss: 0.0051 - val_loss: 0.0135 - 20s/epoch - 20ms/step\n",
      "Epoch 6/200\n",
      "1034/1034 - 21s - loss: 0.0049 - val_loss: 0.0128 - 21s/epoch - 20ms/step\n",
      "Epoch 7/200\n",
      "1034/1034 - 21s - loss: 0.0048 - val_loss: 0.0123 - 21s/epoch - 20ms/step\n",
      "Epoch 8/200\n",
      "1034/1034 - 22s - loss: 0.0047 - val_loss: 0.0118 - 22s/epoch - 21ms/step\n",
      "Epoch 9/200\n",
      "1034/1034 - 20s - loss: 0.0045 - val_loss: 0.0114 - 20s/epoch - 20ms/step\n",
      "Epoch 10/200\n",
      "1034/1034 - 20s - loss: 0.0044 - val_loss: 0.0110 - 20s/epoch - 20ms/step\n",
      "Epoch 11/200\n",
      "1034/1034 - 22s - loss: 0.0044 - val_loss: 0.0110 - 22s/epoch - 21ms/step\n",
      "Epoch 12/200\n",
      "1034/1034 - 23s - loss: 0.0043 - val_loss: 0.0106 - 23s/epoch - 22ms/step\n",
      "Epoch 13/200\n",
      "1034/1034 - 22s - loss: 0.0042 - val_loss: 0.0101 - 22s/epoch - 21ms/step\n",
      "Epoch 14/200\n",
      "1034/1034 - 21s - loss: 0.0041 - val_loss: 0.0097 - 21s/epoch - 20ms/step\n",
      "Epoch 15/200\n",
      "1034/1034 - 21s - loss: 0.0041 - val_loss: 0.0097 - 21s/epoch - 20ms/step\n",
      "Epoch 16/200\n",
      "1034/1034 - 21s - loss: 0.0040 - val_loss: 0.0092 - 21s/epoch - 21ms/step\n",
      "Epoch 17/200\n",
      "1034/1034 - 21s - loss: 0.0040 - val_loss: 0.0092 - 21s/epoch - 21ms/step\n",
      "Epoch 18/200\n",
      "1034/1034 - 21s - loss: 0.0040 - val_loss: 0.0090 - 21s/epoch - 21ms/step\n",
      "Epoch 19/200\n",
      "1034/1034 - 19s - loss: 0.0039 - val_loss: 0.0090 - 19s/epoch - 18ms/step\n",
      "Epoch 20/200\n",
      "1034/1034 - 21s - loss: 0.0039 - val_loss: 0.0093 - 21s/epoch - 21ms/step\n",
      "Epoch 21/200\n",
      "1034/1034 - 21s - loss: 0.0039 - val_loss: 0.0088 - 21s/epoch - 20ms/step\n",
      "Epoch 22/200\n",
      "1034/1034 - 22s - loss: 0.0038 - val_loss: 0.0088 - 22s/epoch - 22ms/step\n",
      "Epoch 23/200\n",
      "1034/1034 - 23s - loss: 0.0038 - val_loss: 0.0089 - 23s/epoch - 23ms/step\n",
      "Epoch 24/200\n",
      "1034/1034 - 23s - loss: 0.0038 - val_loss: 0.0087 - 23s/epoch - 23ms/step\n",
      "Epoch 25/200\n",
      "1034/1034 - 22s - loss: 0.0038 - val_loss: 0.0085 - 22s/epoch - 21ms/step\n",
      "Epoch 26/200\n",
      "1034/1034 - 21s - loss: 0.0037 - val_loss: 0.0085 - 21s/epoch - 21ms/step\n",
      "Epoch 27/200\n",
      "1034/1034 - 21s - loss: 0.0037 - val_loss: 0.0087 - 21s/epoch - 20ms/step\n",
      "Epoch 28/200\n",
      "1034/1034 - 21s - loss: 0.0037 - val_loss: 0.0084 - 21s/epoch - 20ms/step\n",
      "Epoch 29/200\n",
      "1034/1034 - 23s - loss: 0.0037 - val_loss: 0.0084 - 23s/epoch - 22ms/step\n",
      "Epoch 30/200\n",
      "1034/1034 - 21s - loss: 0.0037 - val_loss: 0.0084 - 21s/epoch - 21ms/step\n",
      "Epoch 31/200\n",
      "1034/1034 - 21s - loss: 0.0037 - val_loss: 0.0082 - 21s/epoch - 20ms/step\n",
      "Epoch 32/200\n",
      "1034/1034 - 20s - loss: 0.0036 - val_loss: 0.0083 - 20s/epoch - 20ms/step\n",
      "Epoch 33/200\n",
      "1034/1034 - 20s - loss: 0.0037 - val_loss: 0.0082 - 20s/epoch - 19ms/step\n",
      "Epoch 34/200\n",
      "1034/1034 - 22s - loss: 0.0036 - val_loss: 0.0083 - 22s/epoch - 21ms/step\n",
      "Epoch 35/200\n",
      "1034/1034 - 21s - loss: 0.0036 - val_loss: 0.0083 - 21s/epoch - 20ms/step\n",
      "Epoch 36/200\n",
      "1034/1034 - 21s - loss: 0.0036 - val_loss: 0.0084 - 21s/epoch - 20ms/step\n",
      "Epoch 37/200\n",
      "1034/1034 - 20s - loss: 0.0036 - val_loss: 0.0082 - 20s/epoch - 20ms/step\n",
      "Epoch 38/200\n",
      "1034/1034 - 23s - loss: 0.0036 - val_loss: 0.0082 - 23s/epoch - 22ms/step\n",
      "Epoch 39/200\n",
      "1034/1034 - 21s - loss: 0.0036 - val_loss: 0.0082 - 21s/epoch - 21ms/step\n",
      "Epoch 40/200\n",
      "1034/1034 - 21s - loss: 0.0036 - val_loss: 0.0080 - 21s/epoch - 20ms/step\n",
      "Epoch 41/200\n",
      "1034/1034 - 21s - loss: 0.0036 - val_loss: 0.0083 - 21s/epoch - 21ms/step\n",
      "Epoch 42/200\n",
      "1034/1034 - 21s - loss: 0.0036 - val_loss: 0.0082 - 21s/epoch - 21ms/step\n",
      "Epoch 43/200\n",
      "1034/1034 - 22s - loss: 0.0036 - val_loss: 0.0079 - 22s/epoch - 21ms/step\n",
      "Epoch 44/200\n",
      "1034/1034 - 21s - loss: 0.0035 - val_loss: 0.0079 - 21s/epoch - 20ms/step\n",
      "Epoch 45/200\n",
      "1034/1034 - 21s - loss: 0.0035 - val_loss: 0.0079 - 21s/epoch - 20ms/step\n",
      "Epoch 46/200\n",
      "1034/1034 - 20s - loss: 0.0035 - val_loss: 0.0079 - 20s/epoch - 20ms/step\n",
      "Epoch 47/200\n",
      "1034/1034 - 22s - loss: 0.0035 - val_loss: 0.0079 - 22s/epoch - 21ms/step\n",
      "Epoch 48/200\n",
      "1034/1034 - 20s - loss: 0.0035 - val_loss: 0.0078 - 20s/epoch - 20ms/step\n",
      "Epoch 49/200\n",
      "1034/1034 - 23s - loss: 0.0035 - val_loss: 0.0081 - 23s/epoch - 22ms/step\n",
      "Epoch 50/200\n",
      "1034/1034 - 21s - loss: 0.0035 - val_loss: 0.0078 - 21s/epoch - 20ms/step\n",
      "Epoch 51/200\n",
      "1034/1034 - 22s - loss: 0.0035 - val_loss: 0.0079 - 22s/epoch - 22ms/step\n",
      "Epoch 52/200\n",
      "1034/1034 - 23s - loss: 0.0035 - val_loss: 0.0077 - 23s/epoch - 22ms/step\n",
      "Epoch 53/200\n",
      "1034/1034 - 22s - loss: 0.0035 - val_loss: 0.0076 - 22s/epoch - 21ms/step\n",
      "Epoch 54/200\n",
      "1034/1034 - 22s - loss: 0.0035 - val_loss: 0.0079 - 22s/epoch - 21ms/step\n",
      "Epoch 55/200\n",
      "1034/1034 - 22s - loss: 0.0035 - val_loss: 0.0076 - 22s/epoch - 22ms/step\n",
      "Epoch 56/200\n",
      "1034/1034 - 21s - loss: 0.0034 - val_loss: 0.0077 - 21s/epoch - 21ms/step\n",
      "Epoch 57/200\n",
      "1034/1034 - 22s - loss: 0.0034 - val_loss: 0.0076 - 22s/epoch - 21ms/step\n",
      "Epoch 58/200\n",
      "1034/1034 - 22s - loss: 0.0034 - val_loss: 0.0079 - 22s/epoch - 22ms/step\n",
      "Epoch 59/200\n",
      "1034/1034 - 21s - loss: 0.0034 - val_loss: 0.0075 - 21s/epoch - 20ms/step\n",
      "Epoch 60/200\n",
      "1034/1034 - 22s - loss: 0.0034 - val_loss: 0.0076 - 22s/epoch - 21ms/step\n",
      "Epoch 61/200\n",
      "1034/1034 - 22s - loss: 0.0034 - val_loss: 0.0077 - 22s/epoch - 21ms/step\n",
      "Epoch 62/200\n",
      "1034/1034 - 21s - loss: 0.0034 - val_loss: 0.0075 - 21s/epoch - 20ms/step\n",
      "Epoch 63/200\n",
      "1034/1034 - 22s - loss: 0.0034 - val_loss: 0.0077 - 22s/epoch - 21ms/step\n",
      "Epoch 64/200\n",
      "1034/1034 - 22s - loss: 0.0034 - val_loss: 0.0076 - 22s/epoch - 21ms/step\n",
      "Epoch 65/200\n",
      "1034/1034 - 21s - loss: 0.0034 - val_loss: 0.0075 - 21s/epoch - 20ms/step\n",
      "Epoch 66/200\n",
      "1034/1034 - 20s - loss: 0.0034 - val_loss: 0.0076 - 20s/epoch - 19ms/step\n",
      "Epoch 67/200\n",
      "1034/1034 - 21s - loss: 0.0034 - val_loss: 0.0074 - 21s/epoch - 20ms/step\n",
      "Epoch 68/200\n",
      "1034/1034 - 21s - loss: 0.0034 - val_loss: 0.0076 - 21s/epoch - 20ms/step\n",
      "Epoch 69/200\n",
      "1034/1034 - 22s - loss: 0.0034 - val_loss: 0.0076 - 22s/epoch - 21ms/step\n",
      "Epoch 70/200\n",
      "1034/1034 - 21s - loss: 0.0034 - val_loss: 0.0075 - 21s/epoch - 20ms/step\n",
      "Epoch 71/200\n"
     ]
    }
   ],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    mode='min',\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_X,\n",
    "    train_y,\n",
    "    validation_data=(val_X, val_y),\n",
    "    epochs=epochs,\n",
    "    batch_size=64,\n",
    "    verbose=2,\n",
    "    shuffle=False,\n",
    "    callbacks=[early_stopping],\n",
    ")\n",
    "\n",
    "model.save('./model/lstm_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cff94f-2000-4337-a4d4-be4df9d267fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('./model/lstm_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333d04f6-671d-4141-b5ba-665bfd8beced",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"min val loss:\", min(history.history['val_loss']) )\n",
    "# Make predictions\n",
    "y_pred = model.predict(val_X)\n",
    "print(y_pred.shape)\n",
    "# Evaluate for each time step\n",
    "for i in range(n_steps_out):\n",
    "    mae = mean_absolute_error(val_y[:, i], y_pred[:, i])\n",
    "    print(f\"MAE for step {i+1}: {mae: .4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772ad5cb-e6c5-4280-9ebf-973de0ac3882",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(val_y[0], label='True')\n",
    "plt.plot(y_pred[0], label='Predicted')\n",
    "plt.legend()\n",
    "plt.title('True vs Predicted - Multi-step Forecasting')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f862aae5-be4d-4c25-bf13-c4aeb4018146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "fig1, ax1 = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# Loss plot\n",
    "N = len(history.history[\"loss\"])\n",
    "epoch_range = np.arange(0, N)\n",
    "\n",
    "ax1.plot(epoch_range, history.history[\"loss\"], label='Train MSE loss', marker='o', color='blue')\n",
    "ax1.plot(epoch_range, history.history[\"val_loss\"], label='Validation MSE loss', marker='o', color='orange')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('MAE Loss', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "ax1.legend(loc='upper left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "ax1.set_title('MAE Loss During Training')\n",
    "\n",
    "# RMSE plot (shared x-axis, different y-axis)\n",
    "# ax2 = ax1.twinx()\n",
    "# fig2, ax2 = plt.subplots(figsize=(10, 5))\n",
    "# ax2.plot(epoch_range, history.history[\"unscaled_mse\"], label='Train MSE', color='green')\n",
    "# ax2.plot(epoch_range, history.history[\"val_unscaled_mse\"], label='Validation MSE', color='red')\n",
    "# ax2.set_xlabel('Epochs')\n",
    "# ax2.set_ylabel('Root Mean Squared Error (RMSE)', color='green')\n",
    "# ax2.tick_params(axis='y', labelcolor='green')\n",
    "# ax2.legend(loc='upper left')\n",
    "# plt.grid(True, alpha=0.3)\n",
    "# ax2.set_title('RMSE During Training')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ce1a7e-484a-43c0-915e-757a1b607a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = ['AAPL', 'AMZN', 'GME', 'GOOGL', 'NFLX', 'TSLA', 'MSFT', 'NVDA', 'AMD', 'CRM', 'JPM', 'V', 'MA', 'MSTR', 'DIS', \n",
    "           'BA', 'XOM', 'ORCL', 'META', 'BRK', 'HSBC', 'MS', 'PYPL', 'BAC', 'BLK', 'GS']\n",
    "\n",
    "def split_validation_data(val_X, val_y, stock_lengths):\n",
    "    \"\"\"\n",
    "    Split the combined validation data back into individual stocks.\n",
    "    \n",
    "    Args:\n",
    "        val_X: Combined validation features\n",
    "        val_y: Combined validation targets\n",
    "        stock_lengths: List of number of validation samples for each stock\n",
    "    \"\"\"\n",
    "    val_X_stocks = []\n",
    "    val_y_stocks = []\n",
    "    \n",
    "    start_idx = 0\n",
    "    for length in stock_lengths:\n",
    "        val_X_stocks.append(val_X[start_idx:start_idx + length])\n",
    "        val_y_stocks.append(val_y[start_idx:start_idx + length])\n",
    "        start_idx += length\n",
    "        \n",
    "    return val_X_stocks, val_y_stocks\n",
    "\n",
    "def inverse_transform_price(scaled_data, scaler):\n",
    "    \"\"\"\n",
    "    Convert scaled prices back to actual prices\n",
    "    \"\"\"\n",
    "    # Create a dummy array with zeros for all features\n",
    "    dummy = np.zeros((len(scaled_data), scaler.scale_.shape[0]))\n",
    "    # Put the scaled prices in the correct column (-1 is close price next day)\n",
    "    dummy[:, -1] = scaled_data\n",
    "    # Inverse transform\n",
    "    inverse_transformed = scaler.inverse_transform(dummy)\n",
    "    # Return only the close price column\n",
    "    return inverse_transformed[:, -1]\n",
    "\n",
    "def evaluate_predictions(y_true, y_pred, ticker, scaler):\n",
    "    \"\"\"\n",
    "    Calculate and print evaluation metrics for the predictions\n",
    "    \"\"\"\n",
    "    # Convert scaled values back to actual prices\n",
    "    y_true_actual = inverse_transform_price(y_true, scaler)\n",
    "    y_pred_actual = inverse_transform_price(y_pred, scaler)\n",
    "    \n",
    "    # Calculate metrics on actual prices\n",
    "    mse = mean_squared_error(y_true_actual, y_pred_actual)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true_actual, y_pred_actual)\n",
    "    \n",
    "    print(f\"\\nEvaluation Metrics for {ticker}:\")\n",
    "    print(f\"MSE: {mse:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    \n",
    "    return mse, rmse, mae\n",
    "\n",
    "def plot_predictions(actual_values, predicted_values, ticker, scaler):\n",
    "    \"\"\"\n",
    "    Plot actual vs predicted values in actual prices\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # Convert to actual prices\n",
    "    actual_prices = inverse_transform_price(actual_values, scaler)\n",
    "    predicted_prices = inverse_transform_price(predicted_values, scaler)\n",
    "    \n",
    "    # Create time axis for plotting\n",
    "    time_steps = range(len(actual_prices))\n",
    "    \n",
    "    # Plot\n",
    "    plt.plot(time_steps, actual_prices, label='Actual', color='blue')\n",
    "    plt.plot(time_steps, predicted_prices, label='Predicted', color='red', linestyle='--')\n",
    "    \n",
    "    plt.title(f'{ticker} Stock Price Prediction')\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('Price ($)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Calculate validation set lengths for each stock\n",
    "def get_stock_val_lengths(values_list, train_split=0.8, n_steps_in=14, n_steps_out=5):\n",
    "    \"\"\"\n",
    "    Calculate the number of validation samples for each stock\n",
    "    \"\"\"\n",
    "    val_lengths = []\n",
    "    for values in values_list:\n",
    "        n_train = int(len(values) * train_split)\n",
    "        val_data = values[n_train:]\n",
    "        n_sequences = len(val_data) - n_steps_in - n_steps_out + 1\n",
    "        if n_sequences > 0:\n",
    "            val_lengths.append(n_sequences)\n",
    "    return val_lengths\n",
    "\n",
    "# Calculate validation lengths for each stock\n",
    "val_lengths = get_stock_val_lengths(values_list)\n",
    "\n",
    "# Split validation data back into individual stocks\n",
    "val_X_stocks, val_y_stocks = split_validation_data(val_X, val_y, val_lengths)\n",
    "\n",
    "# Make predictions for each stock\n",
    "predictions_dict = {}\n",
    "metrics_dict = {}\n",
    "\n",
    "for i, ticker in enumerate(tickers):\n",
    "    print(f\"\\nAnalyzing {ticker}...\")\n",
    "    \n",
    "    # Get predictions for valid set\n",
    "    val_predictions = model.predict(val_X_stocks[i], verbose=0)\n",
    "    \n",
    "    # Store predictions\n",
    "    predictions_dict[ticker] = {\n",
    "        'actual': val_y_stocks[i],\n",
    "        'predicted': val_predictions\n",
    "    }\n",
    "    \n",
    "    # Calculate metrics (using first day predictions)\n",
    "    mse, rmse, mae = evaluate_predictions(\n",
    "        val_y_stocks[i][:, 0],  # First day actual\n",
    "        val_predictions[:, 0],   # First day predictions\n",
    "        ticker,\n",
    "        scaler\n",
    "    )\n",
    "    \n",
    "    metrics_dict[ticker] = {\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae\n",
    "    }\n",
    "    \n",
    "    # Plot predictions (first day)\n",
    "    plot_predictions(\n",
    "        val_y_stocks[i][:, 0],    # First day actual\n",
    "        val_predictions[:, 0],     # First day predictions\n",
    "        ticker,\n",
    "        scaler\n",
    "    )\n",
    "\n",
    "# Create a summary DataFrame of metrics\n",
    "metrics_df = pd.DataFrame(metrics_dict).T\n",
    "print(\"\\nSummary of Metrics for All Stocks:\")\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953681e9-df37-4782-aa4f-88e6a8318b46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
