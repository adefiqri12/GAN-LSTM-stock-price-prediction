{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d950ded-1027-4e45-8b25-8f24281348c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T08:46:09.685669Z",
     "iopub.status.busy": "2024-11-25T08:46:09.685257Z",
     "iopub.status.idle": "2024-11-25T08:46:09.691381Z",
     "shell.execute_reply": "2024-11-25T08:46:09.690301Z",
     "shell.execute_reply.started": "2024-11-25T08:46:09.685643Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 440 Âµs (started: 2024-11-25 08:46:09 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# \"\"\"\n",
    "# predicting a sequence of future prices (`n_steps_out`) instead of just one.\n",
    "\n",
    "# Benefits of This Approach\n",
    "# Utilizes Temporal Patterns: The model learns dependencies across multiple future time steps.\n",
    "# More Robust Predictions: Predicting a sequence captures trends better than single-step predictions.\n",
    "# Real-World Applicability: Multi-step forecasting is closer to real-world use cases, such as portfolio management.\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba2dc6c-a4c3-4858-8746-10fd762da60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt -q\n",
    "# !pip install ipython-autotime -q\n",
    "# # !pip install tensorflow-addons -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f4333d-89b9-4595-af27-fe9811842cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from keras.layers import Input, Dense, Dropout, BatchNormalization, Reshape, Flatten, Bidirectional, LSTM\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autotime\n",
    "\n",
    "print(\"tensorflow version:\", tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "tf.test.is_built_with_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a131d3ef-8a5d-4d18-b97b-1636caf46390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter\n",
    "# n_steps_in: Number of input time steps\n",
    "# n_steps_out: Number of days to predict ahead\n",
    "\n",
    "n_steps_in = 14\n",
    "n_steps_out = 5\n",
    "epochs = 200\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f71067-767a-46a5-80a7-215a24bdabf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_processed_data(data_dir='processed_data', df_name='', array_name=''):\n",
    "    \"\"\"\n",
    "    Load processed DataFrames list and numpy arrays from files\n",
    "    \"\"\"\n",
    "    data_path = Path(data_dir)\n",
    "    \n",
    "    # Load list of DataFrames\n",
    "    with open(data_path / df_name, 'rb') as f:\n",
    "        processed_dfs = joblib.load(f)\n",
    "    \n",
    "    # Load numpy arrays\n",
    "    with open(data_path / array_name, 'rb') as f:\n",
    "        values_list = joblib.load(f)\n",
    "    \n",
    "    return processed_dfs, values_list\n",
    "\n",
    "processed_dfs, values_list = load_processed_data(df_name='processed_dfs.pkl', array_name='values_array.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e426c2fb-215a-40bf-8d5b-a4a9769378ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "values_list = []\n",
    "for i in range(len(processed_dfs)):\n",
    "    # processed_dfs[i].drop(processed_dfs[i].columns[1:5], axis=1, inplace=True)\n",
    "    processed_dfs[i].drop(columns=['Volume', 'RSI', 'WR','Price Momentum', 'Volume Momentum', 'CCI', 'Ulcer Index'],inplace=True)\n",
    "    x = processed_dfs[i].values\n",
    "    values_list.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc07535b-1dc8-461d-a7ed-f32c1a976ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad6ccf5-ec0c-4604-bd56-1d8aa11e6768",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_stock_data(values_list, n_steps_in=14, n_steps_out=5, train_split=0.8):\n",
    "    \"\"\"\n",
    "    Preprocess multiple stock datasets for LSTM training.\n",
    "    \n",
    "    Args:\n",
    "        values_list: List of numpy arrays containing stock data with technical indicators\n",
    "        n_steps_in: Number of lookback days\n",
    "        n_steps_out: Number of prediction days\n",
    "        train_split: Train/validation split ratio\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (train_X, train_y, val_X, val_y, global_scaler)\n",
    "    \"\"\"\n",
    "    # 1. Global scaling across all stocks\n",
    "    global_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    combined_values = np.vstack(values_list)\n",
    "    scaled_combined = global_scaler.fit_transform(combined_values)\n",
    "    \n",
    "    # 2. Split back into individual stocks\n",
    "    scaled_values_list = []\n",
    "    start_idx = 0\n",
    "    for values in values_list:\n",
    "        scaled_values_list.append(scaled_combined[start_idx:start_idx + len(values)])\n",
    "        start_idx += len(values)\n",
    "    \n",
    "    # 3. Create sequences for each stock\n",
    "    train_X_list, train_y_list = [], []\n",
    "    val_X_list, val_y_list = [], []\n",
    "    \n",
    "    for scaled_values in scaled_values_list:\n",
    "        # Remove 'Close Next Day' from features (last column)\n",
    "        features = scaled_values[:, :-1]  # All columns except the last one\n",
    "        targets = scaled_values[:, -1]    # Only the last column\n",
    "        \n",
    "        # Split into train/validation\n",
    "        n_train = int(len(features) * train_split)\n",
    "        \n",
    "        # Ensure we have enough data for both training and validation\n",
    "        if n_train <= n_steps_in + n_steps_out:\n",
    "            print(f\"Warning: Stock with {len(features)} samples is too short for meaningful splitting\")\n",
    "            continue\n",
    "            \n",
    "        # Split features and targets\n",
    "        train_features = features[:n_train]\n",
    "        train_targets = targets[:n_train]\n",
    "        val_features = features[n_train:]\n",
    "        val_targets = targets[n_train:]\n",
    "        \n",
    "        # Create sequences\n",
    "        if len(train_features) > n_steps_in + n_steps_out:\n",
    "            train_X, train_y = create_sequences(train_features, train_targets, n_steps_in, n_steps_out)\n",
    "            train_X_list.append(train_X)\n",
    "            train_y_list.append(train_y)\n",
    "            \n",
    "        if len(val_features) > n_steps_in + n_steps_out:\n",
    "            val_X, val_y = create_sequences(val_features, val_targets, n_steps_in, n_steps_out)\n",
    "            val_X_list.append(val_X)\n",
    "            val_y_list.append(val_y)\n",
    "    \n",
    "    # 4. Combine all sequences\n",
    "    train_X = np.vstack(train_X_list)\n",
    "    train_y = np.vstack(train_y_list)\n",
    "    val_X = np.vstack(val_X_list)\n",
    "    val_y = np.vstack(val_y_list)\n",
    "    \n",
    "    print(f\"Training shapes: X={train_X.shape}, y={train_y.shape}\")\n",
    "    print(f\"Validation shapes: X={val_X.shape}, y={val_y.shape}\")\n",
    "    print(f\"Number of features: {train_X.shape[2]}\")\n",
    "    \n",
    "    return train_X, train_y, val_X, val_y, global_scaler\n",
    "\n",
    "def create_sequences(features, targets, n_steps_in, n_steps_out):\n",
    "    \"\"\"\n",
    "    Generate synchronized sequences for LSTM input features and output targets.\n",
    "    \n",
    "    Args:\n",
    "        features: Scaled feature data (numpy array)\n",
    "        targets: Scaled target data (numpy array)\n",
    "        n_steps_in: Number of input time steps\n",
    "        n_steps_out: Number of output time steps\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (X sequences, y sequences)\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    \n",
    "    # Ensure we have enough data for sequence creation\n",
    "    if len(features) < n_steps_in + n_steps_out:\n",
    "        raise ValueError(\"Data length is too short for the specified sequence lengths\")\n",
    "    \n",
    "    for i in range(len(features) - n_steps_in - n_steps_out + 1):\n",
    "        # Input sequence (n_steps_in days of all features)\n",
    "        seq_x = features[i:(i + n_steps_in)]\n",
    "        # Output sequence (next n_steps_out days of target variable)\n",
    "        seq_y = targets[(i + n_steps_in):(i + n_steps_in + n_steps_out)]\n",
    "        \n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "train_X, train_y, val_X, val_y, scaler = preprocess_stock_data(values_list, n_steps_in, n_steps_out, train_split=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d086cc3e-82d4-4fa7-9116-4c2a0598bef2",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84421eaf-7f87-4409-b27d-1bafd23e1691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow_addons as tfa\n",
    "\n",
    "# Cyclical learning rate\n",
    "# steps_per_epoch = len(train_X) // batch_size\n",
    "# n = 2\n",
    "# cycle = 12\n",
    "# epochs = cycle * n * 2\n",
    "# print(epochs)\n",
    "\n",
    "# clr = tfa.optimizers.TriangularCyclicalLearningRate(\n",
    "#     initial_learning_rate=1e-4,\n",
    "#     maximal_learning_rate=1e-2,\n",
    "#     step_size = n * steps_per_epoch)\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "def build_generator(n_steps_in, n_features, n_steps_out):    \n",
    "    inputs = Input(shape=(n_steps_in, n_features))\n",
    "    lstm = LSTM(200, activation='relu')(inputs)\n",
    "    outputs = Dense(n_steps_out)(lstm)\n",
    "    model = Model(inputs, outputs, name='generator')\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "generator = build_generator(n_steps_in=n_steps_in, n_features=train_X.shape[2], n_steps_out=n_steps_out)\n",
    "tf.keras.utils.plot_model(generator, show_shapes=True, show_layer_names=True, to_file='generator.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b05e4f-16fd-4b06-a1d9-3d85b0409bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(n_steps_out, n_features):\n",
    "    inputs = Input(shape=(n_steps_out, n_features))\n",
    "    lstm = LSTM(100, activation='relu', return_sequences=True)(inputs)\n",
    "    lstm2 = LSTM(50, activation='relu')(lstm)\n",
    "    dense = Dense(1, activation='sigmoid')(lstm2)  # Output: probability of real or fake\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs, dense, name='discriminator')\n",
    "    optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "    loss = ['binary_crossentropy']\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "discriminator = build_discriminator(n_steps_out=n_steps_out, n_features=train_X.shape[2])\n",
    "tf.keras.utils.plot_model(discriminator, show_shapes=True, show_layer_names=True, to_file='discriminator.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbe8365-cfd0-49f1-b875-ed2ca6ea8a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gan(generator, discriminator):\n",
    "    discriminator.trainable = False\n",
    "\n",
    "    gan_input = Input(shape=(,))\n",
    "    x = generator(gan_input)\n",
    "    gan_output = discriminator(x)\n",
    "\n",
    "    gan = tf.keras.Model(gan_input, gan_output)\n",
    "    gan.compile(optimizer='adam', loss='mae')\n",
    "    return gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55a2135-96f3-404e-9999-b0bdf0a115cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T08:46:18.137533Z",
     "iopub.status.busy": "2024-11-25T08:46:18.137133Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1034/1034 - 24s - loss: 0.0358 - val_loss: 0.0195 - 24s/epoch - 23ms/step\n",
      "Epoch 2/200\n",
      "1034/1034 - 22s - loss: 0.0076 - val_loss: 0.0160 - 22s/epoch - 21ms/step\n",
      "Epoch 3/200\n",
      "1034/1034 - 21s - loss: 0.0059 - val_loss: 0.0148 - 21s/epoch - 21ms/step\n",
      "Epoch 4/200\n",
      "1034/1034 - 21s - loss: 0.0054 - val_loss: 0.0140 - 21s/epoch - 20ms/step\n",
      "Epoch 5/200\n",
      "1034/1034 - 20s - loss: 0.0051 - val_loss: 0.0135 - 20s/epoch - 20ms/step\n",
      "Epoch 6/200\n",
      "1034/1034 - 21s - loss: 0.0049 - val_loss: 0.0128 - 21s/epoch - 20ms/step\n",
      "Epoch 7/200\n",
      "1034/1034 - 21s - loss: 0.0048 - val_loss: 0.0123 - 21s/epoch - 20ms/step\n",
      "Epoch 8/200\n",
      "1034/1034 - 22s - loss: 0.0047 - val_loss: 0.0118 - 22s/epoch - 21ms/step\n",
      "Epoch 9/200\n",
      "1034/1034 - 20s - loss: 0.0045 - val_loss: 0.0114 - 20s/epoch - 20ms/step\n",
      "Epoch 10/200\n",
      "1034/1034 - 20s - loss: 0.0044 - val_loss: 0.0110 - 20s/epoch - 20ms/step\n",
      "Epoch 11/200\n",
      "1034/1034 - 22s - loss: 0.0044 - val_loss: 0.0110 - 22s/epoch - 21ms/step\n",
      "Epoch 12/200\n",
      "1034/1034 - 23s - loss: 0.0043 - val_loss: 0.0106 - 23s/epoch - 22ms/step\n",
      "Epoch 13/200\n",
      "1034/1034 - 22s - loss: 0.0042 - val_loss: 0.0101 - 22s/epoch - 21ms/step\n",
      "Epoch 14/200\n",
      "1034/1034 - 21s - loss: 0.0041 - val_loss: 0.0097 - 21s/epoch - 20ms/step\n",
      "Epoch 15/200\n",
      "1034/1034 - 21s - loss: 0.0041 - val_loss: 0.0097 - 21s/epoch - 20ms/step\n",
      "Epoch 16/200\n",
      "1034/1034 - 21s - loss: 0.0040 - val_loss: 0.0092 - 21s/epoch - 21ms/step\n",
      "Epoch 17/200\n",
      "1034/1034 - 21s - loss: 0.0040 - val_loss: 0.0092 - 21s/epoch - 21ms/step\n",
      "Epoch 18/200\n",
      "1034/1034 - 21s - loss: 0.0040 - val_loss: 0.0090 - 21s/epoch - 21ms/step\n",
      "Epoch 19/200\n",
      "1034/1034 - 19s - loss: 0.0039 - val_loss: 0.0090 - 19s/epoch - 18ms/step\n",
      "Epoch 20/200\n",
      "1034/1034 - 21s - loss: 0.0039 - val_loss: 0.0093 - 21s/epoch - 21ms/step\n",
      "Epoch 21/200\n",
      "1034/1034 - 21s - loss: 0.0039 - val_loss: 0.0088 - 21s/epoch - 20ms/step\n",
      "Epoch 22/200\n",
      "1034/1034 - 22s - loss: 0.0038 - val_loss: 0.0088 - 22s/epoch - 22ms/step\n",
      "Epoch 23/200\n",
      "1034/1034 - 23s - loss: 0.0038 - val_loss: 0.0089 - 23s/epoch - 23ms/step\n",
      "Epoch 24/200\n",
      "1034/1034 - 23s - loss: 0.0038 - val_loss: 0.0087 - 23s/epoch - 23ms/step\n",
      "Epoch 25/200\n",
      "1034/1034 - 22s - loss: 0.0038 - val_loss: 0.0085 - 22s/epoch - 21ms/step\n",
      "Epoch 26/200\n",
      "1034/1034 - 21s - loss: 0.0037 - val_loss: 0.0085 - 21s/epoch - 21ms/step\n",
      "Epoch 27/200\n",
      "1034/1034 - 21s - loss: 0.0037 - val_loss: 0.0087 - 21s/epoch - 20ms/step\n",
      "Epoch 28/200\n",
      "1034/1034 - 21s - loss: 0.0037 - val_loss: 0.0084 - 21s/epoch - 20ms/step\n",
      "Epoch 29/200\n",
      "1034/1034 - 23s - loss: 0.0037 - val_loss: 0.0084 - 23s/epoch - 22ms/step\n",
      "Epoch 30/200\n",
      "1034/1034 - 21s - loss: 0.0037 - val_loss: 0.0084 - 21s/epoch - 21ms/step\n",
      "Epoch 31/200\n",
      "1034/1034 - 21s - loss: 0.0037 - val_loss: 0.0082 - 21s/epoch - 20ms/step\n",
      "Epoch 32/200\n",
      "1034/1034 - 20s - loss: 0.0036 - val_loss: 0.0083 - 20s/epoch - 20ms/step\n",
      "Epoch 33/200\n",
      "1034/1034 - 20s - loss: 0.0037 - val_loss: 0.0082 - 20s/epoch - 19ms/step\n",
      "Epoch 34/200\n",
      "1034/1034 - 22s - loss: 0.0036 - val_loss: 0.0083 - 22s/epoch - 21ms/step\n",
      "Epoch 35/200\n",
      "1034/1034 - 21s - loss: 0.0036 - val_loss: 0.0083 - 21s/epoch - 20ms/step\n",
      "Epoch 36/200\n",
      "1034/1034 - 21s - loss: 0.0036 - val_loss: 0.0084 - 21s/epoch - 20ms/step\n",
      "Epoch 37/200\n",
      "1034/1034 - 20s - loss: 0.0036 - val_loss: 0.0082 - 20s/epoch - 20ms/step\n",
      "Epoch 38/200\n",
      "1034/1034 - 23s - loss: 0.0036 - val_loss: 0.0082 - 23s/epoch - 22ms/step\n",
      "Epoch 39/200\n",
      "1034/1034 - 21s - loss: 0.0036 - val_loss: 0.0082 - 21s/epoch - 21ms/step\n",
      "Epoch 40/200\n",
      "1034/1034 - 21s - loss: 0.0036 - val_loss: 0.0080 - 21s/epoch - 20ms/step\n",
      "Epoch 41/200\n",
      "1034/1034 - 21s - loss: 0.0036 - val_loss: 0.0083 - 21s/epoch - 21ms/step\n",
      "Epoch 42/200\n",
      "1034/1034 - 21s - loss: 0.0036 - val_loss: 0.0082 - 21s/epoch - 21ms/step\n",
      "Epoch 43/200\n",
      "1034/1034 - 22s - loss: 0.0036 - val_loss: 0.0079 - 22s/epoch - 21ms/step\n",
      "Epoch 44/200\n",
      "1034/1034 - 21s - loss: 0.0035 - val_loss: 0.0079 - 21s/epoch - 20ms/step\n",
      "Epoch 45/200\n",
      "1034/1034 - 21s - loss: 0.0035 - val_loss: 0.0079 - 21s/epoch - 20ms/step\n",
      "Epoch 46/200\n",
      "1034/1034 - 20s - loss: 0.0035 - val_loss: 0.0079 - 20s/epoch - 20ms/step\n",
      "Epoch 47/200\n",
      "1034/1034 - 22s - loss: 0.0035 - val_loss: 0.0079 - 22s/epoch - 21ms/step\n",
      "Epoch 48/200\n",
      "1034/1034 - 20s - loss: 0.0035 - val_loss: 0.0078 - 20s/epoch - 20ms/step\n",
      "Epoch 49/200\n",
      "1034/1034 - 23s - loss: 0.0035 - val_loss: 0.0081 - 23s/epoch - 22ms/step\n",
      "Epoch 50/200\n",
      "1034/1034 - 21s - loss: 0.0035 - val_loss: 0.0078 - 21s/epoch - 20ms/step\n",
      "Epoch 51/200\n",
      "1034/1034 - 22s - loss: 0.0035 - val_loss: 0.0079 - 22s/epoch - 22ms/step\n",
      "Epoch 52/200\n",
      "1034/1034 - 23s - loss: 0.0035 - val_loss: 0.0077 - 23s/epoch - 22ms/step\n",
      "Epoch 53/200\n",
      "1034/1034 - 22s - loss: 0.0035 - val_loss: 0.0076 - 22s/epoch - 21ms/step\n",
      "Epoch 54/200\n",
      "1034/1034 - 22s - loss: 0.0035 - val_loss: 0.0079 - 22s/epoch - 21ms/step\n",
      "Epoch 55/200\n",
      "1034/1034 - 22s - loss: 0.0035 - val_loss: 0.0076 - 22s/epoch - 22ms/step\n",
      "Epoch 56/200\n",
      "1034/1034 - 21s - loss: 0.0034 - val_loss: 0.0077 - 21s/epoch - 21ms/step\n",
      "Epoch 57/200\n",
      "1034/1034 - 22s - loss: 0.0034 - val_loss: 0.0076 - 22s/epoch - 21ms/step\n",
      "Epoch 58/200\n",
      "1034/1034 - 22s - loss: 0.0034 - val_loss: 0.0079 - 22s/epoch - 22ms/step\n",
      "Epoch 59/200\n",
      "1034/1034 - 21s - loss: 0.0034 - val_loss: 0.0075 - 21s/epoch - 20ms/step\n",
      "Epoch 60/200\n",
      "1034/1034 - 22s - loss: 0.0034 - val_loss: 0.0076 - 22s/epoch - 21ms/step\n",
      "Epoch 61/200\n",
      "1034/1034 - 22s - loss: 0.0034 - val_loss: 0.0077 - 22s/epoch - 21ms/step\n",
      "Epoch 62/200\n",
      "1034/1034 - 21s - loss: 0.0034 - val_loss: 0.0075 - 21s/epoch - 20ms/step\n",
      "Epoch 63/200\n",
      "1034/1034 - 22s - loss: 0.0034 - val_loss: 0.0077 - 22s/epoch - 21ms/step\n",
      "Epoch 64/200\n",
      "1034/1034 - 22s - loss: 0.0034 - val_loss: 0.0076 - 22s/epoch - 21ms/step\n",
      "Epoch 65/200\n",
      "1034/1034 - 21s - loss: 0.0034 - val_loss: 0.0075 - 21s/epoch - 20ms/step\n",
      "Epoch 66/200\n",
      "1034/1034 - 20s - loss: 0.0034 - val_loss: 0.0076 - 20s/epoch - 19ms/step\n",
      "Epoch 67/200\n",
      "1034/1034 - 21s - loss: 0.0034 - val_loss: 0.0074 - 21s/epoch - 20ms/step\n",
      "Epoch 68/200\n",
      "1034/1034 - 21s - loss: 0.0034 - val_loss: 0.0076 - 21s/epoch - 20ms/step\n",
      "Epoch 69/200\n",
      "1034/1034 - 22s - loss: 0.0034 - val_loss: 0.0076 - 22s/epoch - 21ms/step\n",
      "Epoch 70/200\n",
      "1034/1034 - 21s - loss: 0.0034 - val_loss: 0.0075 - 21s/epoch - 20ms/step\n",
      "Epoch 71/200\n"
     ]
    }
   ],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    mode='min',\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_X,\n",
    "    train_y,\n",
    "    validation_data=(val_X, val_y),\n",
    "    epochs=epochs,\n",
    "    batch_size=64,\n",
    "    verbose=2,\n",
    "    shuffle=False,\n",
    "    callbacks=[early_stopping],\n",
    ")\n",
    "\n",
    "model.save('./model/lstm_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cff94f-2000-4337-a4d4-be4df9d267fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('./model/lstm_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333d04f6-671d-4141-b5ba-665bfd8beced",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"min val loss:\", min(history.history['val_loss']) )\n",
    "# Make predictions\n",
    "y_pred = model.predict(val_X)\n",
    "print(y_pred.shape)\n",
    "# Evaluate for each time step\n",
    "for i in range(n_steps_out):\n",
    "    mae = mean_absolute_error(val_y[:, i], y_pred[:, i])\n",
    "    print(f\"MAE for step {i+1}: {mae: .4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772ad5cb-e6c5-4280-9ebf-973de0ac3882",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(val_y[0], label='True')\n",
    "plt.plot(y_pred[0], label='Predicted')\n",
    "plt.legend()\n",
    "plt.title('True vs Predicted - Multi-step Forecasting')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f862aae5-be4d-4c25-bf13-c4aeb4018146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "fig1, ax1 = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# Loss plot\n",
    "N = len(history.history[\"loss\"])\n",
    "epoch_range = np.arange(0, N)\n",
    "\n",
    "ax1.plot(epoch_range, history.history[\"loss\"], label='Train MSE loss', marker='o', color='blue')\n",
    "ax1.plot(epoch_range, history.history[\"val_loss\"], label='Validation MSE loss', marker='o', color='orange')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('MAE Loss', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "ax1.legend(loc='upper left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "ax1.set_title('MAE Loss During Training')\n",
    "\n",
    "# RMSE plot (shared x-axis, different y-axis)\n",
    "# ax2 = ax1.twinx()\n",
    "# fig2, ax2 = plt.subplots(figsize=(10, 5))\n",
    "# ax2.plot(epoch_range, history.history[\"unscaled_mse\"], label='Train MSE', color='green')\n",
    "# ax2.plot(epoch_range, history.history[\"val_unscaled_mse\"], label='Validation MSE', color='red')\n",
    "# ax2.set_xlabel('Epochs')\n",
    "# ax2.set_ylabel('Root Mean Squared Error (RMSE)', color='green')\n",
    "# ax2.tick_params(axis='y', labelcolor='green')\n",
    "# ax2.legend(loc='upper left')\n",
    "# plt.grid(True, alpha=0.3)\n",
    "# ax2.set_title('RMSE During Training')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ce1a7e-484a-43c0-915e-757a1b607a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = ['AAPL', 'AMZN', 'GME', 'GOOGL', 'NFLX', 'TSLA', 'MSFT', 'NVDA', 'AMD', 'CRM', 'JPM', 'V', 'MA', 'MSTR', 'DIS', \n",
    "           'BA', 'XOM', 'ORCL', 'META', 'BRK', 'HSBC', 'MS', 'PYPL', 'BAC', 'BLK', 'GS']\n",
    "\n",
    "def split_validation_data(val_X, val_y, stock_lengths):\n",
    "    \"\"\"\n",
    "    Split the combined validation data back into individual stocks.\n",
    "    \n",
    "    Args:\n",
    "        val_X: Combined validation features\n",
    "        val_y: Combined validation targets\n",
    "        stock_lengths: List of number of validation samples for each stock\n",
    "    \"\"\"\n",
    "    val_X_stocks = []\n",
    "    val_y_stocks = []\n",
    "    \n",
    "    start_idx = 0\n",
    "    for length in stock_lengths:\n",
    "        val_X_stocks.append(val_X[start_idx:start_idx + length])\n",
    "        val_y_stocks.append(val_y[start_idx:start_idx + length])\n",
    "        start_idx += length\n",
    "        \n",
    "    return val_X_stocks, val_y_stocks\n",
    "\n",
    "def inverse_transform_price(scaled_data, scaler):\n",
    "    \"\"\"\n",
    "    Convert scaled prices back to actual prices\n",
    "    \"\"\"\n",
    "    # Create a dummy array with zeros for all features\n",
    "    dummy = np.zeros((len(scaled_data), scaler.scale_.shape[0]))\n",
    "    # Put the scaled prices in the correct column (-1 is close price next day)\n",
    "    dummy[:, -1] = scaled_data\n",
    "    # Inverse transform\n",
    "    inverse_transformed = scaler.inverse_transform(dummy)\n",
    "    # Return only the close price column\n",
    "    return inverse_transformed[:, -1]\n",
    "\n",
    "def evaluate_predictions(y_true, y_pred, ticker, scaler):\n",
    "    \"\"\"\n",
    "    Calculate and print evaluation metrics for the predictions\n",
    "    \"\"\"\n",
    "    # Convert scaled values back to actual prices\n",
    "    y_true_actual = inverse_transform_price(y_true, scaler)\n",
    "    y_pred_actual = inverse_transform_price(y_pred, scaler)\n",
    "    \n",
    "    # Calculate metrics on actual prices\n",
    "    mse = mean_squared_error(y_true_actual, y_pred_actual)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true_actual, y_pred_actual)\n",
    "    \n",
    "    print(f\"\\nEvaluation Metrics for {ticker}:\")\n",
    "    print(f\"MSE: {mse:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    \n",
    "    return mse, rmse, mae\n",
    "\n",
    "def plot_predictions(actual_values, predicted_values, ticker, scaler):\n",
    "    \"\"\"\n",
    "    Plot actual vs predicted values in actual prices\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # Convert to actual prices\n",
    "    actual_prices = inverse_transform_price(actual_values, scaler)\n",
    "    predicted_prices = inverse_transform_price(predicted_values, scaler)\n",
    "    \n",
    "    # Create time axis for plotting\n",
    "    time_steps = range(len(actual_prices))\n",
    "    \n",
    "    # Plot\n",
    "    plt.plot(time_steps, actual_prices, label='Actual', color='blue')\n",
    "    plt.plot(time_steps, predicted_prices, label='Predicted', color='red', linestyle='--')\n",
    "    \n",
    "    plt.title(f'{ticker} Stock Price Prediction')\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('Price ($)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Calculate validation set lengths for each stock\n",
    "def get_stock_val_lengths(values_list, train_split=0.8, n_steps_in=14, n_steps_out=5):\n",
    "    \"\"\"\n",
    "    Calculate the number of validation samples for each stock\n",
    "    \"\"\"\n",
    "    val_lengths = []\n",
    "    for values in values_list:\n",
    "        n_train = int(len(values) * train_split)\n",
    "        val_data = values[n_train:]\n",
    "        n_sequences = len(val_data) - n_steps_in - n_steps_out + 1\n",
    "        if n_sequences > 0:\n",
    "            val_lengths.append(n_sequences)\n",
    "    return val_lengths\n",
    "\n",
    "# Calculate validation lengths for each stock\n",
    "val_lengths = get_stock_val_lengths(values_list)\n",
    "\n",
    "# Split validation data back into individual stocks\n",
    "val_X_stocks, val_y_stocks = split_validation_data(val_X, val_y, val_lengths)\n",
    "\n",
    "# Make predictions for each stock\n",
    "predictions_dict = {}\n",
    "metrics_dict = {}\n",
    "\n",
    "for i, ticker in enumerate(tickers):\n",
    "    print(f\"\\nAnalyzing {ticker}...\")\n",
    "    \n",
    "    # Get predictions for valid set\n",
    "    val_predictions = model.predict(val_X_stocks[i], verbose=0)\n",
    "    \n",
    "    # Store predictions\n",
    "    predictions_dict[ticker] = {\n",
    "        'actual': val_y_stocks[i],\n",
    "        'predicted': val_predictions\n",
    "    }\n",
    "    \n",
    "    # Calculate metrics (using first day predictions)\n",
    "    mse, rmse, mae = evaluate_predictions(\n",
    "        val_y_stocks[i][:, 0],  # First day actual\n",
    "        val_predictions[:, 0],   # First day predictions\n",
    "        ticker,\n",
    "        scaler\n",
    "    )\n",
    "    \n",
    "    metrics_dict[ticker] = {\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae\n",
    "    }\n",
    "    \n",
    "    # Plot predictions (first day)\n",
    "    plot_predictions(\n",
    "        val_y_stocks[i][:, 0],    # First day actual\n",
    "        val_predictions[:, 0],     # First day predictions\n",
    "        ticker,\n",
    "        scaler\n",
    "    )\n",
    "\n",
    "# Create a summary DataFrame of metrics\n",
    "metrics_df = pd.DataFrame(metrics_dict).T\n",
    "print(\"\\nSummary of Metrics for All Stocks:\")\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953681e9-df37-4782-aa4f-88e6a8318b46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
